{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from preprocess import filter_pos, process_text, remove_nt, lemma_pattern, lemmatize_word, adv_to_adj\n",
    "from vader import get_sentiment\n",
    "from pain_points import get_frequent, get_negative_tokens, create_token_match_columns, process_token_df\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pycommon.warehouse.load_queries import acquire_all_review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquire all review data with skip and limit types: <class 'NoneType'> <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "#port = os.getenv(\"MONGO_PORT\") if os.getenv(\"MONGO_PORT\") is not None else 27017 # MONGO_PORT defines the port number. \n",
    "port= 27017\n",
    "mongo_client = MongoClient('localhost', port) # mongo is always the host. Again, docker handles this dns resolution.\n",
    "\n",
    "# And we're good! mongo is ready to be used. Most of the methods in pycommon/warehouse need you to \n",
    "# pass in the mongoclient. \n",
    "\n",
    "reviews = acquire_all_review_data(\n",
    "        mongo_client, \n",
    "        datetime.datetime(2001,12,1,0,0).timestamp(), # from\n",
    "        datetime.datetime(2018,12,1,0,0).timestamp(), # to\n",
    "        \"SimpangAsia\",\n",
    "        \"Yelp\"\n",
    "    )\n",
    "\n",
    "reviews_array = []\n",
    "for review in reviews:\n",
    "    reviews_array.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"timestamp\": [reviews_array[i].timestamp for i in range(0,len(reviews_array))],\n",
    "    \"source_id\": [reviews_array[i].source_id for i in range(0,len(reviews_array))],\n",
    "    \"business_id\": [reviews_array[i].business_id for i in range(0,len(reviews_array))],\n",
    "    \"review_content\": [reviews_array[i].content for i in range(0,len(reviews_array))],\n",
    "    \"review_rating\": [reviews_array[i].rating for i in range(0,len(reviews_array))],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retains only adjectives and adverbs for reviews\n",
    "df['review_tokens'] = df['review_content'].apply(filter_pos)\n",
    "# Makes lowercase, removes punctuation and stopwords, and lemmatizes remaining words\n",
    "df['review_tokens'] = df['review_tokens'].apply(process_text)\n",
    "# removes the word 'nt'\n",
    "df['review_tokens'] = df['review_tokens'].apply(remove_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting tokens\n",
    "most_freq = get_frequent(df['review_tokens'],500)\n",
    "neg_corp = get_negative_tokens(most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iechi\\Desktop\\Files\\Work\\Coding Projects\\BBB\\src\\nlp\\pain_points.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  token_df['neg_sentence'] = token_df['review_content'].apply(lambda x: get_neg_sentence(neg_token_list[index], x))\n",
      "C:\\Users\\iechi\\Desktop\\Files\\Work\\Coding Projects\\BBB\\src\\nlp\\pain_points.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  token_df.dropna(inplace=True)\n",
      "C:\\Users\\iechi\\Desktop\\Files\\Work\\Coding Projects\\BBB\\src\\nlp\\pain_points.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  token_df['df_len'] = len(token_df)\n",
      "C:\\Users\\iechi\\Desktop\\Files\\Work\\Coding Projects\\BBB\\src\\nlp\\pain_points.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  token_df['token'] = neg_token_list[index]\n"
     ]
    }
   ],
   "source": [
    "create_token_match_columns(neg_corp, df)\n",
    "token_df = process_token_df(neg_corp, df)\n",
    "token_df.sort_values(['df_len','token'], ascending = False, inplace=True)\n",
    "token_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.drop(neg_corp, axis=1, inplace=True)\n",
    "token_df.drop(['level_0', 'index','review_content'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>neg_sentence</th>\n",
       "      <th>df_len</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>first everlasting Indonesia Malaysia happy yum...</td>\n",
       "      <td>It's insanely spicy! I felt the fire in my mo...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>favorite first new subsequent locality bad eas...</td>\n",
       "      <td>Let's start with the bad: parking</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>4</td>\n",
       "      <td>great thorough tender bad fast hot enough odd ...</td>\n",
       "      <td>The bad side is that some dishes came out ver...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>worst ever bad thorough mild extra hot true le...</td>\n",
       "      <td>very bad services and food is not good</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>absolute horrible sure easier bland reasonable...</td>\n",
       "      <td>It was so bad, he filed a report with the LA ...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp source_id  business_id  review_rating  \\\n",
       "0 2018-08-06      Yelp  SimpangAsia              5   \n",
       "1 2017-10-01      Yelp  SimpangAsia              5   \n",
       "2 2017-12-07      Yelp  SimpangAsia              4   \n",
       "3 2018-11-07      Yelp  SimpangAsia              1   \n",
       "4 2016-06-11      Yelp  SimpangAsia              1   \n",
       "\n",
       "                                       review_tokens  \\\n",
       "0  first everlasting Indonesia Malaysia happy yum...   \n",
       "1  favorite first new subsequent locality bad eas...   \n",
       "2  great thorough tender bad fast hot enough odd ...   \n",
       "3  worst ever bad thorough mild extra hot true le...   \n",
       "4  absolute horrible sure easier bland reasonable...   \n",
       "\n",
       "                                        neg_sentence  df_len token  \n",
       "0   It's insanely spicy! I felt the fire in my mo...      48   bad  \n",
       "1                  Let's start with the bad: parking      48   bad  \n",
       "2   The bad side is that some dishes came out ver...      48   bad  \n",
       "3             very bad services and food is not good      48   bad  \n",
       "4   It was so bad, he filed a report with the LA ...      48   bad  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_adj_matcher_pos(sentence):\n",
    "    # matches each noun in the setence to the adjective and returns a list of tuples containing [(noun, adj)] \n",
    "    doc = nlp(sentence)\n",
    "    noun_adj_pairs = []\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('NOUN','PROPN','VERB'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ == 'ADJ' or doc[j].pos_ == 'ADV':\n",
    "                noun_adj_pairs.append((token.text,doc[j].text))\n",
    "                break\n",
    "    return noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_noun_matcher_pos(sentence):\n",
    "    # matches each noun in the setence to the adjective and returns a list of tuples containing [(noun, adj)] \n",
    "    doc = nlp(sentence)\n",
    "    noun_adj_pairs = []\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.pos_ not in ('ADJ', 'ADV'):\n",
    "            continue\n",
    "        for j in range(i+1,len(doc)):\n",
    "            if doc[j].pos_ in ('NOUN','PROPN','VERB'):\n",
    "                noun_adj_pairs.append((token.text,doc[j].text))\n",
    "                break\n",
    "    return noun_adj_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def verb_adv_matcher_pos(sentence):\n",
    "    # matches each verb in the setence to the adverb and returns a list of tuples containing [(verb, adv)] \n",
    "#    doc = nlp(sentence)\n",
    "#    verb_adv_pairs = []\n",
    "#    for i,token in enumerate(doc):\n",
    "#        if token.pos_ not in ('VERB'):\n",
    "#            continue\n",
    "#        for j in range(i+1,len(doc)):\n",
    "#            if doc[j].pos_ == 'ADV':\n",
    "#                verb_adv_pairs.append((token.text,doc[j].text))\n",
    "#                break\n",
    "#    return verb_adv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pos(sentence):\n",
    "    noun_list = noun_adj_matcher_pos(sentence)\n",
    "    # verb_list = verb_adv_matcher_pos(sentence)\n",
    "    # return noun_list+verb_list\n",
    "    return noun_list+adj_noun_matcher_pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>neg_sentence</th>\n",
       "      <th>df_len</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>first everlasting Indonesia Malaysia happy yum...</td>\n",
       "      <td>It's insanely spicy! I felt the fire in my mo...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>favorite first new subsequent locality bad eas...</td>\n",
       "      <td>Let's start with the bad: parking</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>4</td>\n",
       "      <td>great thorough tender bad fast hot enough odd ...</td>\n",
       "      <td>The bad side is that some dishes came out ver...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>worst ever bad thorough mild extra hot true le...</td>\n",
       "      <td>very bad services and food is not good</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>absolute horrible sure easier bland reasonable...</td>\n",
       "      <td>It was so bad, he filed a report with the LA ...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp source_id  business_id  review_rating  \\\n",
       "0 2018-08-06      Yelp  SimpangAsia              5   \n",
       "1 2017-10-01      Yelp  SimpangAsia              5   \n",
       "2 2017-12-07      Yelp  SimpangAsia              4   \n",
       "3 2018-11-07      Yelp  SimpangAsia              1   \n",
       "4 2016-06-11      Yelp  SimpangAsia              1   \n",
       "\n",
       "                                       review_tokens  \\\n",
       "0  first everlasting Indonesia Malaysia happy yum...   \n",
       "1  favorite first new subsequent locality bad eas...   \n",
       "2  great thorough tender bad fast hot enough odd ...   \n",
       "3  worst ever bad thorough mild extra hot true le...   \n",
       "4  absolute horrible sure easier bland reasonable...   \n",
       "\n",
       "                                        neg_sentence  df_len token  \n",
       "0   It's insanely spicy! I felt the fire in my mo...      48   bad  \n",
       "1                  Let's start with the bad: parking      48   bad  \n",
       "2   The bad side is that some dishes came out ver...      48   bad  \n",
       "3             very bad services and food is not good      48   bad  \n",
       "4   It was so bad, he filed a report with the LA ...      48   bad  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_described(sentence, token):\n",
    "    # returns one tuple containing the description as well as the word being described from neg_sentence\n",
    "    tuple_list = match_pos(sentence)\n",
    "    for word, description in tuple_list:\n",
    "        if description == token:\n",
    "            return word, description\n",
    "    # return None, token\n",
    "    return 'Failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_described_cols(df):\n",
    "    df['word_described'] = 'NA'\n",
    "    for index in range(0, len(df)):\n",
    "        df['word_described'][index] = get_word_described(df['neg_sentence'].apply(lambda x: lemmatize_word(adv_to_adj(lemma_pattern(x))))[index], df['token'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is not as effective\n",
    "#\n",
    "# def create_word_described_cols(df):\n",
    "#    df['word_described'] = 'NA'\n",
    "#    for index in range(0, len(df)):\n",
    "#        df['word_described'][index] = get_word_described(df['review_tokens'][index], df['token'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\iechi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "create_word_described_cols(token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>neg_sentence</th>\n",
       "      <th>df_len</th>\n",
       "      <th>token</th>\n",
       "      <th>word_described</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-06</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>first everlasting Indonesia Malaysia happy yum...</td>\n",
       "      <td>It's insanely spicy! I felt the fire in my mo...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>5</td>\n",
       "      <td>favorite first new subsequent locality bad eas...</td>\n",
       "      <td>Let's start with the bad: parking</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "      <td>(let, bad)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>4</td>\n",
       "      <td>great thorough tender bad fast hot enough odd ...</td>\n",
       "      <td>The bad side is that some dishes came out ver...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>worst ever bad thorough mild extra hot true le...</td>\n",
       "      <td>very bad services and food is not good</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>Yelp</td>\n",
       "      <td>SimpangAsia</td>\n",
       "      <td>1</td>\n",
       "      <td>absolute horrible sure easier bland reasonable...</td>\n",
       "      <td>It was so bad, he filed a report with the LA ...</td>\n",
       "      <td>48</td>\n",
       "      <td>bad</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp source_id  business_id  review_rating  \\\n",
       "0 2018-08-06      Yelp  SimpangAsia              5   \n",
       "1 2017-10-01      Yelp  SimpangAsia              5   \n",
       "2 2017-12-07      Yelp  SimpangAsia              4   \n",
       "3 2018-11-07      Yelp  SimpangAsia              1   \n",
       "4 2016-06-11      Yelp  SimpangAsia              1   \n",
       "\n",
       "                                       review_tokens  \\\n",
       "0  first everlasting Indonesia Malaysia happy yum...   \n",
       "1  favorite first new subsequent locality bad eas...   \n",
       "2  great thorough tender bad fast hot enough odd ...   \n",
       "3  worst ever bad thorough mild extra hot true le...   \n",
       "4  absolute horrible sure easier bland reasonable...   \n",
       "\n",
       "                                        neg_sentence  df_len token  \\\n",
       "0   It's insanely spicy! I felt the fire in my mo...      48   bad   \n",
       "1                  Let's start with the bad: parking      48   bad   \n",
       "2   The bad side is that some dishes came out ver...      48   bad   \n",
       "3             very bad services and food is not good      48   bad   \n",
       "4   It was so bad, he filed a report with the LA ...      48   bad   \n",
       "\n",
       "  word_described  \n",
       "0         Failed  \n",
       "1     (let, bad)  \n",
       "2         Failed  \n",
       "3         Failed  \n",
       "4         Failed  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Failed                    203\n",
       "(parking, difficult)        4\n",
       "(parking, hard)             4\n",
       "('s, hard)                  3\n",
       "(parking, terrible)         3\n",
       "(was, terrible)             3\n",
       "(service, bad)              2\n",
       "(had, worst)                2\n",
       "(was, weird)                2\n",
       "('s, odd)                   2\n",
       "(meat, tough)               1\n",
       "(rames, horrible)           1\n",
       "(ambience, terrible)        1\n",
       "(boy, difficult)            1\n",
       "(what, strange)             1\n",
       "(be, hard)                  1\n",
       "(strikes, strange)          1\n",
       "(eat, sick)                 1\n",
       "(valet, odd)                1\n",
       "(brought, strange)          1\n",
       "(have, low)                 1\n",
       "(food, poor)                1\n",
       "(is, worst)                 1\n",
       "(were, bad)                 1\n",
       "(based, bad)                1\n",
       "(complaint, bad)            1\n",
       "('d, bad)                   1\n",
       "(service, worse)            1\n",
       "(groceries, hard)           1\n",
       "(place, empty)              1\n",
       "                         ... \n",
       "(curry, hard)               1\n",
       "(was, insane)               1\n",
       "(found, difficult)          1\n",
       "(parking, worse)            1\n",
       "(boiled, weird)             1\n",
       "(happened, damn)            1\n",
       "(parking, rude)             1\n",
       "(am, sorry)                 1\n",
       "(felt, bad)                 1\n",
       "(also, annoy)               1\n",
       "(note, difficult)           1\n",
       "(afternoon, empty)          1\n",
       "(waitress, odd)             1\n",
       "(expanded, low)             1\n",
       "('re, sick)                 1\n",
       "(smoothie, nasty)           1\n",
       "(will, disappoint)          1\n",
       "(gado, weird)               1\n",
       "(is, negative)              1\n",
       "('m, weak)                  1\n",
       "(was, difficult)            1\n",
       "(service, terrible)         1\n",
       "(experience, negative)      1\n",
       "(being, difficult)          1\n",
       "(dish, crazy)               1\n",
       "(was, sad)                  1\n",
       "(is, bad)                   1\n",
       "(were, crazy)               1\n",
       "(tables, difficult)         1\n",
       "(parking, bad)              1\n",
       "Name: word_described, Length: 111, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df['word_described'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple noun-adj / verb-adv matching does not work! Sentences are too complicated to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the accuracy of our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(row):\n",
    "    print ('Token: ', token_df.iloc[row]['token'])\n",
    "    print ('Sentence: ', token_df.iloc[row]['neg_sentence'])\n",
    "    print ('Lemmatized: ', lemmatize_word(adv_to_adj(lemma_pattern(token_df.iloc[row]['neg_sentence']))))\n",
    "    print ('Processed: ', process_text(token_df.iloc[row]['neg_sentence']))\n",
    "    print ('Tuple: ', token_df.iloc[row]['word_described'])\n",
    "    print ('Lemmatized Description: ', [word for word in nlp(lemmatize_word(adv_to_adj(lemma_pattern(token_df.iloc[row]['neg_sentence'])))) if word.pos_ in ('NOUN', 'PNOUN', 'VERB')])\n",
    "    print ('Processed Description: ', [word for word in nlp(process_text(token_df.iloc[row]['neg_sentence'])) if word.pos_ in ('NOUN', 'PNOUN', 'VERB')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  bad\n",
      "Sentence:  Let's start with the bad: parking\n",
      "Lemmatized:  let's start with the bad: park\n",
      "Processed:  let start bad park\n",
      "Tuple:  ('let', 'bad')\n",
      "Lemmatized Description:  [let, start, park]\n",
      "Processed Description:  [let, start, park]\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ\n"
     ]
    }
   ],
   "source": [
    "# HERE IS ONE PROBLEM!\n",
    "doc = nlp('bad')\n",
    "for token in doc:\n",
    "    print (token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIOUS METHODS FAILED BECAUSE DESCRIPTION IS CHECKED ONLY AFTER THE WORD\n",
    "# THIS IS SPACY's RULE BASED MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", None, pattern)\n",
    "\n",
    "doc = nlp(u\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS NLTK'S ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\iechi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# GETTING ALL ENTITIES\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "            if type(i) == Tree:\n",
    "                    current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            elif current_chunk:\n",
    "                    named_entity = \" \".join(current_chunk)\n",
    "                    if named_entity not in continuous_chunk:\n",
    "                            continuous_chunk.append(named_entity)\n",
    "                            current_chunk = []\n",
    "            else:\n",
    "                    continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Label for Entity\n",
    "def get_entity_label(sentence):\n",
    "    for sent in nltk.sent_tokenize(sentence):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"food was very bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_continuous_chunks(sentence))\n",
    "get_entity_label(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEMS THAT USING ENTITY RECOGNITION DOES NOT WORK AS WELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
